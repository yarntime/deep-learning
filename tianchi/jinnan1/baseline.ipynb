{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.model_selection import KFold, RepeatedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from scipy import sparse\n",
    "import warnings\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import log_loss\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns',None)\n",
    "pd.set_option('max_colwidth',100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./jinnan_round1_train_20181227.csv\", encoding = 'gb18030')\n",
    "test = pd.read_csv(\"./jinnan_round1_testA_20181227.csv\", encoding = 'gb18030')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除类别唯一的特征\n",
    "for df in [train, test]:\n",
    "    df.drop(['B3', 'B13', 'A13', 'A18', 'A23'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'A1', 0.9863896848137536)\n",
      "(u'A2', 0.9699140401146131)\n",
      "(u'A3', 0.9570200573065902)\n",
      "(u'A4', 0.9570200573065902)\n",
      "(u'B2', 0.9842406876790831)\n"
     ]
    }
   ],
   "source": [
    "# 删除缺失率超过90%的列\n",
    "good_cols = list(train.columns)\n",
    "for col in train.columns:\n",
    "    rate = train[col].value_counts(normalize=True, dropna=False).values[0]\n",
    "    if rate > 0.95:\n",
    "        good_cols.remove(col)\n",
    "        print(col,rate)\n",
    "\n",
    "# 删除异常值\n",
    "train = train[train['score']>0.87]\n",
    "        \n",
    "train = train[good_cols]\n",
    "good_cols.remove('score')\n",
    "test  = test[good_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并数据集\n",
    "target = train['score']\n",
    "del train['score']\n",
    "data = pd.concat([train,test],axis=0,ignore_index=True)\n",
    "data = data.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeTranSecond(t):\n",
    "    try:\n",
    "        t,m,s=t.split(\":\")\n",
    "    except:\n",
    "        if t=='1900/1/9 7:00':\n",
    "            return 7*3600/3600\n",
    "        elif t=='1900/1/1 2:30':\n",
    "            return (2*3600+30*60)/3600\n",
    "        elif t==-1:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    try:\n",
    "        tm = (int(t)*3600+int(m)*60+int(s))/3600\n",
    "    except:\n",
    "        return (30*60)/3600\n",
    "    \n",
    "    return tm\n",
    "for f in ['A5','A7','A9','A11','A14','A16','A24','A26','B5','B7']:\n",
    "    try:\n",
    "        data[f] = data[f].apply(timeTranSecond)\n",
    "    except:\n",
    "        print(f,'应该在前面被删除了！')\n",
    "\n",
    "def getDuration(se):\n",
    "    try:\n",
    "        sh,sm,eh,em=re.findall(r\"\\d+\\.?\\d*\",se)\n",
    "    except:\n",
    "        if se == -1:\n",
    "            return -1 \n",
    "        \n",
    "    try:\n",
    "        if int(sh)>int(eh):\n",
    "            tm = (int(eh)*3600+int(em)*60-int(sm)*60-int(sh)*3600)/3600 + 24\n",
    "        else:\n",
    "            tm = (int(eh)*3600+int(em)*60-int(sm)*60-int(sh)*3600)/3600\n",
    "    except:\n",
    "        if se=='19:-20:05':\n",
    "            return 1\n",
    "        elif se=='15:00-1600':\n",
    "            return 1\n",
    "    \n",
    "    return tm\n",
    "for f in ['A20','A28','B4','B9','B10','B11']:\n",
    "    data[f] = data.apply(lambda df: getDuration(df[f]), axis=1)\n",
    "    \n",
    "    \n",
    "def getID(s):\n",
    "    t,m=s.split(\"_\")\n",
    "    return m\n",
    "\n",
    "data['ID'] = data.apply(lambda df: getID(df['sample id']), axis=1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'A5', u'A6', u'A7', u'A8', u'A9', u'A10', u'A11', u'A12', u'A14', u'A15', u'A16', u'A17', u'A19', u'A20', u'A21', u'A22', u'A24', u'A25', u'A26', u'A27', u'A28', u'B1', u'B4', u'B5', u'B6', u'B7', u'B8', u'B9', u'B10', u'B11', u'B12', u'B14', 'ID']\n"
     ]
    }
   ],
   "source": [
    "cate_columns = [f for f in data.columns if f != 'sample id']\n",
    "print(cate_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label encoder\n",
    "for f in cate_columns:\n",
    "    data[f] = data[f].map(dict(zip(data[f].unique(), range(0, data[f].nunique()))))\n",
    "train = data[:train.shape[0]]\n",
    "test  = data[train.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['target'] = target\n",
    "train['intTarget'] = pd.cut(train['target'], 5, labels=False)\n",
    "train = pd.get_dummies(train, columns=['intTarget'])\n",
    "li = ['intTarget_0.0','intTarget_1.0','intTarget_2.0','intTarget_3.0','intTarget_4.0']\n",
    "mean_features = []\n",
    "\n",
    "for f1 in cate_columns:\n",
    "    rate = train[f1].value_counts(normalize=True, dropna=False).values[0]\n",
    "    if rate < 0.50:\n",
    "        for f2 in li:\n",
    "            col_name = f1+\"_\"+f2+'_mean'\n",
    "            mean_features.append(col_name)\n",
    "            order_label = train.groupby([f1])[f2].mean()\n",
    "            for df in [train, test]:\n",
    "                df[col_name] = df[f].map(order_label)\n",
    "\n",
    "train.drop(li, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1381, 118)\n",
      "(150, 118)\n"
     ]
    }
   ],
   "source": [
    "train.drop(['sample id', 'target'], axis=1, inplace=True)\n",
    "test = test[train.columns]\n",
    "X_train = train.values\n",
    "y_train = target.values\n",
    "X_test = test.values\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A5</th>\n",
       "      <th>A6</th>\n",
       "      <th>A7</th>\n",
       "      <th>A8</th>\n",
       "      <th>A9</th>\n",
       "      <th>A10</th>\n",
       "      <th>A11</th>\n",
       "      <th>A12</th>\n",
       "      <th>A14</th>\n",
       "      <th>A15</th>\n",
       "      <th>A16</th>\n",
       "      <th>A17</th>\n",
       "      <th>A19</th>\n",
       "      <th>A20</th>\n",
       "      <th>A21</th>\n",
       "      <th>A22</th>\n",
       "      <th>A24</th>\n",
       "      <th>A25</th>\n",
       "      <th>A26</th>\n",
       "      <th>A27</th>\n",
       "      <th>A28</th>\n",
       "      <th>B1</th>\n",
       "      <th>B4</th>\n",
       "      <th>B5</th>\n",
       "      <th>B6</th>\n",
       "      <th>B7</th>\n",
       "      <th>B8</th>\n",
       "      <th>B9</th>\n",
       "      <th>B10</th>\n",
       "      <th>B11</th>\n",
       "      <th>B12</th>\n",
       "      <th>B14</th>\n",
       "      <th>ID</th>\n",
       "      <th>A5_intTarget_0.0_mean</th>\n",
       "      <th>A5_intTarget_1.0_mean</th>\n",
       "      <th>A5_intTarget_2.0_mean</th>\n",
       "      <th>A5_intTarget_3.0_mean</th>\n",
       "      <th>A5_intTarget_4.0_mean</th>\n",
       "      <th>A6_intTarget_0.0_mean</th>\n",
       "      <th>A6_intTarget_1.0_mean</th>\n",
       "      <th>A6_intTarget_2.0_mean</th>\n",
       "      <th>A6_intTarget_3.0_mean</th>\n",
       "      <th>A6_intTarget_4.0_mean</th>\n",
       "      <th>A9_intTarget_0.0_mean</th>\n",
       "      <th>A9_intTarget_1.0_mean</th>\n",
       "      <th>A9_intTarget_2.0_mean</th>\n",
       "      <th>A9_intTarget_3.0_mean</th>\n",
       "      <th>A9_intTarget_4.0_mean</th>\n",
       "      <th>A10_intTarget_0.0_mean</th>\n",
       "      <th>A10_intTarget_1.0_mean</th>\n",
       "      <th>A10_intTarget_2.0_mean</th>\n",
       "      <th>A10_intTarget_3.0_mean</th>\n",
       "      <th>A10_intTarget_4.0_mean</th>\n",
       "      <th>A11_intTarget_0.0_mean</th>\n",
       "      <th>A11_intTarget_1.0_mean</th>\n",
       "      <th>A11_intTarget_2.0_mean</th>\n",
       "      <th>A11_intTarget_3.0_mean</th>\n",
       "      <th>A11_intTarget_4.0_mean</th>\n",
       "      <th>A12_intTarget_0.0_mean</th>\n",
       "      <th>A12_intTarget_1.0_mean</th>\n",
       "      <th>A12_intTarget_2.0_mean</th>\n",
       "      <th>A12_intTarget_3.0_mean</th>\n",
       "      <th>A12_intTarget_4.0_mean</th>\n",
       "      <th>A14_intTarget_0.0_mean</th>\n",
       "      <th>A14_intTarget_1.0_mean</th>\n",
       "      <th>A14_intTarget_2.0_mean</th>\n",
       "      <th>A14_intTarget_3.0_mean</th>\n",
       "      <th>A14_intTarget_4.0_mean</th>\n",
       "      <th>A16_intTarget_0.0_mean</th>\n",
       "      <th>A16_intTarget_1.0_mean</th>\n",
       "      <th>A16_intTarget_2.0_mean</th>\n",
       "      <th>A16_intTarget_3.0_mean</th>\n",
       "      <th>A16_intTarget_4.0_mean</th>\n",
       "      <th>A17_intTarget_0.0_mean</th>\n",
       "      <th>A17_intTarget_1.0_mean</th>\n",
       "      <th>A17_intTarget_2.0_mean</th>\n",
       "      <th>A17_intTarget_3.0_mean</th>\n",
       "      <th>A17_intTarget_4.0_mean</th>\n",
       "      <th>A24_intTarget_0.0_mean</th>\n",
       "      <th>A24_intTarget_1.0_mean</th>\n",
       "      <th>A24_intTarget_2.0_mean</th>\n",
       "      <th>A24_intTarget_3.0_mean</th>\n",
       "      <th>A24_intTarget_4.0_mean</th>\n",
       "      <th>A25_intTarget_0.0_mean</th>\n",
       "      <th>A25_intTarget_1.0_mean</th>\n",
       "      <th>A25_intTarget_2.0_mean</th>\n",
       "      <th>A25_intTarget_3.0_mean</th>\n",
       "      <th>A25_intTarget_4.0_mean</th>\n",
       "      <th>A26_intTarget_0.0_mean</th>\n",
       "      <th>A26_intTarget_1.0_mean</th>\n",
       "      <th>A26_intTarget_2.0_mean</th>\n",
       "      <th>A26_intTarget_3.0_mean</th>\n",
       "      <th>A26_intTarget_4.0_mean</th>\n",
       "      <th>A27_intTarget_0.0_mean</th>\n",
       "      <th>A27_intTarget_1.0_mean</th>\n",
       "      <th>A27_intTarget_2.0_mean</th>\n",
       "      <th>A27_intTarget_3.0_mean</th>\n",
       "      <th>A27_intTarget_4.0_mean</th>\n",
       "      <th>B5_intTarget_0.0_mean</th>\n",
       "      <th>B5_intTarget_1.0_mean</th>\n",
       "      <th>B5_intTarget_2.0_mean</th>\n",
       "      <th>B5_intTarget_3.0_mean</th>\n",
       "      <th>B5_intTarget_4.0_mean</th>\n",
       "      <th>B6_intTarget_0.0_mean</th>\n",
       "      <th>B6_intTarget_1.0_mean</th>\n",
       "      <th>B6_intTarget_2.0_mean</th>\n",
       "      <th>B6_intTarget_3.0_mean</th>\n",
       "      <th>B6_intTarget_4.0_mean</th>\n",
       "      <th>B7_intTarget_0.0_mean</th>\n",
       "      <th>B7_intTarget_1.0_mean</th>\n",
       "      <th>B7_intTarget_2.0_mean</th>\n",
       "      <th>B7_intTarget_3.0_mean</th>\n",
       "      <th>B7_intTarget_4.0_mean</th>\n",
       "      <th>ID_intTarget_0.0_mean</th>\n",
       "      <th>ID_intTarget_1.0_mean</th>\n",
       "      <th>ID_intTarget_2.0_mean</th>\n",
       "      <th>ID_intTarget_3.0_mean</th>\n",
       "      <th>ID_intTarget_4.0_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.438596</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.070175</td>\n",
       "      <td>0.121951</td>\n",
       "      <td>0.341463</td>\n",
       "      <td>0.317073</td>\n",
       "      <td>0.121951</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.177743</td>\n",
       "      <td>0.272025</td>\n",
       "      <td>0.387944</td>\n",
       "      <td>0.092736</td>\n",
       "      <td>0.052550</td>\n",
       "      <td>0.121951</td>\n",
       "      <td>0.341463</td>\n",
       "      <td>0.317073</td>\n",
       "      <td>0.121951</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.177500</td>\n",
       "      <td>0.265000</td>\n",
       "      <td>0.392500</td>\n",
       "      <td>0.095000</td>\n",
       "      <td>0.057500</td>\n",
       "      <td>0.121951</td>\n",
       "      <td>0.341463</td>\n",
       "      <td>0.317073</td>\n",
       "      <td>0.121951</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.121951</td>\n",
       "      <td>0.341463</td>\n",
       "      <td>0.317073</td>\n",
       "      <td>0.121951</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.195402</td>\n",
       "      <td>0.258621</td>\n",
       "      <td>0.425287</td>\n",
       "      <td>0.074713</td>\n",
       "      <td>0.028736</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.413043</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.180272</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.394558</td>\n",
       "      <td>0.102041</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.130597</td>\n",
       "      <td>0.309701</td>\n",
       "      <td>0.373134</td>\n",
       "      <td>0.093284</td>\n",
       "      <td>0.078358</td>\n",
       "      <td>0.160243</td>\n",
       "      <td>0.292089</td>\n",
       "      <td>0.383367</td>\n",
       "      <td>0.093306</td>\n",
       "      <td>0.064909</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.292308</td>\n",
       "      <td>0.407692</td>\n",
       "      <td>0.084615</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.107383</td>\n",
       "      <td>0.325503</td>\n",
       "      <td>0.402685</td>\n",
       "      <td>0.097315</td>\n",
       "      <td>0.067114</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.296154</td>\n",
       "      <td>0.403846</td>\n",
       "      <td>0.084615</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.141593</td>\n",
       "      <td>0.296460</td>\n",
       "      <td>0.401180</td>\n",
       "      <td>0.082596</td>\n",
       "      <td>0.067847</td>\n",
       "      <td>0.130268</td>\n",
       "      <td>0.295019</td>\n",
       "      <td>0.406130</td>\n",
       "      <td>0.084291</td>\n",
       "      <td>0.076628</td>\n",
       "      <td>0.130268</td>\n",
       "      <td>0.295019</td>\n",
       "      <td>0.406130</td>\n",
       "      <td>0.084291</td>\n",
       "      <td>0.076628</td>\n",
       "      <td>0.160294</td>\n",
       "      <td>0.302941</td>\n",
       "      <td>0.388235</td>\n",
       "      <td>0.080882</td>\n",
       "      <td>0.060294</td>\n",
       "      <td>0.128405</td>\n",
       "      <td>0.311284</td>\n",
       "      <td>0.389105</td>\n",
       "      <td>0.085603</td>\n",
       "      <td>0.077821</td>\n",
       "      <td>0.159851</td>\n",
       "      <td>0.284387</td>\n",
       "      <td>0.390335</td>\n",
       "      <td>0.100372</td>\n",
       "      <td>0.055762</td>\n",
       "      <td>0.129771</td>\n",
       "      <td>0.305344</td>\n",
       "      <td>0.396947</td>\n",
       "      <td>0.080153</td>\n",
       "      <td>0.076336</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.271132</td>\n",
       "      <td>0.408293</td>\n",
       "      <td>0.095694</td>\n",
       "      <td>0.066986</td>\n",
       "      <td>0.121622</td>\n",
       "      <td>0.310811</td>\n",
       "      <td>0.387387</td>\n",
       "      <td>0.090090</td>\n",
       "      <td>0.081081</td>\n",
       "      <td>0.154088</td>\n",
       "      <td>0.273585</td>\n",
       "      <td>0.410377</td>\n",
       "      <td>0.091195</td>\n",
       "      <td>0.064465</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.279070</td>\n",
       "      <td>0.186047</td>\n",
       "      <td>0.441860</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.258454</td>\n",
       "      <td>0.415459</td>\n",
       "      <td>0.079710</td>\n",
       "      <td>0.062802</td>\n",
       "      <td>0.219512</td>\n",
       "      <td>0.219512</td>\n",
       "      <td>0.341463</td>\n",
       "      <td>0.170732</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.211382</td>\n",
       "      <td>0.252033</td>\n",
       "      <td>0.390244</td>\n",
       "      <td>0.097561</td>\n",
       "      <td>0.032520</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.121739</td>\n",
       "      <td>0.226087</td>\n",
       "      <td>0.452174</td>\n",
       "      <td>0.121739</td>\n",
       "      <td>0.069565</td>\n",
       "      <td>0.196970</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.439394</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.189873</td>\n",
       "      <td>0.291139</td>\n",
       "      <td>0.430380</td>\n",
       "      <td>0.037975</td>\n",
       "      <td>0.050633</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.355556</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.167273</td>\n",
       "      <td>0.294545</td>\n",
       "      <td>0.385455</td>\n",
       "      <td>0.087273</td>\n",
       "      <td>0.050909</td>\n",
       "      <td>0.152091</td>\n",
       "      <td>0.304183</td>\n",
       "      <td>0.418251</td>\n",
       "      <td>0.076046</td>\n",
       "      <td>0.041825</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.308756</td>\n",
       "      <td>0.368664</td>\n",
       "      <td>0.092166</td>\n",
       "      <td>0.078341</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.182857</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.348571</td>\n",
       "      <td>0.102857</td>\n",
       "      <td>0.068571</td>\n",
       "      <td>0.151832</td>\n",
       "      <td>0.293194</td>\n",
       "      <td>0.356021</td>\n",
       "      <td>0.104712</td>\n",
       "      <td>0.062827</td>\n",
       "      <td>0.177215</td>\n",
       "      <td>0.291139</td>\n",
       "      <td>0.341772</td>\n",
       "      <td>0.107595</td>\n",
       "      <td>0.075949</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.287500</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.106250</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>0.471429</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.354037</td>\n",
       "      <td>0.105590</td>\n",
       "      <td>0.074534</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.354037</td>\n",
       "      <td>0.105590</td>\n",
       "      <td>0.074534</td>\n",
       "      <td>0.227723</td>\n",
       "      <td>0.277228</td>\n",
       "      <td>0.356436</td>\n",
       "      <td>0.069307</td>\n",
       "      <td>0.049505</td>\n",
       "      <td>0.178344</td>\n",
       "      <td>0.267516</td>\n",
       "      <td>0.363057</td>\n",
       "      <td>0.108280</td>\n",
       "      <td>0.076433</td>\n",
       "      <td>0.168605</td>\n",
       "      <td>0.275194</td>\n",
       "      <td>0.393411</td>\n",
       "      <td>0.096899</td>\n",
       "      <td>0.050388</td>\n",
       "      <td>0.176101</td>\n",
       "      <td>0.264151</td>\n",
       "      <td>0.371069</td>\n",
       "      <td>0.113208</td>\n",
       "      <td>0.075472</td>\n",
       "      <td>0.165049</td>\n",
       "      <td>0.237864</td>\n",
       "      <td>0.441748</td>\n",
       "      <td>0.101942</td>\n",
       "      <td>0.038835</td>\n",
       "      <td>0.174863</td>\n",
       "      <td>0.278689</td>\n",
       "      <td>0.382514</td>\n",
       "      <td>0.092896</td>\n",
       "      <td>0.071038</td>\n",
       "      <td>0.128205</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.193548</td>\n",
       "      <td>0.354839</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.112150</td>\n",
       "      <td>0.299065</td>\n",
       "      <td>0.336449</td>\n",
       "      <td>0.177570</td>\n",
       "      <td>0.056075</td>\n",
       "      <td>0.134021</td>\n",
       "      <td>0.302405</td>\n",
       "      <td>0.422680</td>\n",
       "      <td>0.079038</td>\n",
       "      <td>0.054983</td>\n",
       "      <td>0.145161</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>0.381720</td>\n",
       "      <td>0.086022</td>\n",
       "      <td>0.091398</td>\n",
       "      <td>0.236842</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.370370</td>\n",
       "      <td>0.129630</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.165680</td>\n",
       "      <td>0.272189</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.100592</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A5  A6  A7  A8  A9  A10  A11  A12  A14  A15  A16  A17  A19  A20  A21  A22  \\\n",
       "0   0   0   0   0   0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "1   1   1   0   0   1    1    1    1    1    1    1    1    1    1    0    0   \n",
       "2   1   1   0   0   1    2    1    1    1    1    1    1    1    0    0    0   \n",
       "3   2   0   0   0   2    0    2    0    2    0    2    0    1    0    0    1   \n",
       "4   3   1   0   0   3    1    3    1    3    1    3    1    1    1    0    0   \n",
       "\n",
       "   A24  A25  A26  A27  A28  B1  B4  B5  B6  B7  B8  B9  B10  B11  B12  B14  \\\n",
       "0    0    0    0    0    0   0   0   0   0   0   0   0    0    0    0    0   \n",
       "1    1    1    1    1    1   1   0   1   1   1   0   0    0    1    1    0   \n",
       "2    1    2    1    1    1   1   0   1   1   2   0   0    0    1    1    0   \n",
       "3    2    3    2    2    1   2   0   2   0   3   0   0    0    0    0    0   \n",
       "4    3    1    3    1    1   1   0   3   1   4   0   0    0    1    1    1   \n",
       "\n",
       "   ID  A5_intTarget_0.0_mean  A5_intTarget_1.0_mean  A5_intTarget_2.0_mean  \\\n",
       "0   0               0.208333               0.208333               0.458333   \n",
       "1   1               0.130597               0.309701               0.373134   \n",
       "2   2               0.279070               0.186047               0.441860   \n",
       "3   3               0.182857               0.285714               0.348571   \n",
       "4   4               0.193548               0.354839               0.322581   \n",
       "\n",
       "   A5_intTarget_3.0_mean  A5_intTarget_4.0_mean  A6_intTarget_0.0_mean  \\\n",
       "0               0.083333               0.041667               0.157895   \n",
       "1               0.093284               0.078358               0.160243   \n",
       "2               0.069767               0.023256               0.173913   \n",
       "3               0.102857               0.068571               0.151832   \n",
       "4               0.129032               0.000000               0.312500   \n",
       "\n",
       "   A6_intTarget_1.0_mean  A6_intTarget_2.0_mean  A6_intTarget_3.0_mean  \\\n",
       "0               0.210526               0.438596               0.105263   \n",
       "1               0.292089               0.383367               0.093306   \n",
       "2               0.260870               0.434783               0.086957   \n",
       "3               0.293194               0.356021               0.104712   \n",
       "4               0.125000               0.375000               0.062500   \n",
       "\n",
       "   A6_intTarget_4.0_mean  A9_intTarget_0.0_mean  A9_intTarget_1.0_mean  \\\n",
       "0               0.070175               0.121951               0.341463   \n",
       "1               0.064909               0.130769               0.292308   \n",
       "2               0.043478               0.214286               0.214286   \n",
       "3               0.062827               0.177215               0.291139   \n",
       "4               0.062500               0.093750               0.312500   \n",
       "\n",
       "   A9_intTarget_2.0_mean  A9_intTarget_3.0_mean  A9_intTarget_4.0_mean  \\\n",
       "0               0.317073               0.121951               0.048780   \n",
       "1               0.407692               0.084615               0.076923   \n",
       "2               0.357143               0.166667               0.047619   \n",
       "3               0.341772               0.107595               0.075949   \n",
       "4               0.437500               0.093750               0.062500   \n",
       "\n",
       "   A10_intTarget_0.0_mean  A10_intTarget_1.0_mean  A10_intTarget_2.0_mean  \\\n",
       "0                0.177743                0.272025                0.387944   \n",
       "1                0.107383                0.325503                0.402685   \n",
       "2                0.173913                0.258454                0.415459   \n",
       "3                0.136364                0.318182                0.363636   \n",
       "4                     NaN                     NaN                     NaN   \n",
       "\n",
       "   A10_intTarget_3.0_mean  A10_intTarget_4.0_mean  A11_intTarget_0.0_mean  \\\n",
       "0                0.092736                0.052550                0.121951   \n",
       "1                0.097315                0.067114                0.130769   \n",
       "2                0.079710                0.062802                0.219512   \n",
       "3                0.136364                0.045455                0.175000   \n",
       "4                     NaN                     NaN                0.093750   \n",
       "\n",
       "   A11_intTarget_1.0_mean  A11_intTarget_2.0_mean  A11_intTarget_3.0_mean  \\\n",
       "0                0.341463                0.317073                0.121951   \n",
       "1                0.296154                0.403846                0.084615   \n",
       "2                0.219512                0.341463                0.170732   \n",
       "3                0.287500                0.350000                0.106250   \n",
       "4                0.312500                0.437500                0.093750   \n",
       "\n",
       "   A11_intTarget_4.0_mean  A12_intTarget_0.0_mean  A12_intTarget_1.0_mean  \\\n",
       "0                0.048780                0.177500                0.265000   \n",
       "1                0.076923                0.141593                0.296460   \n",
       "2                0.048780                0.211382                0.252033   \n",
       "3                0.075000                0.142857                0.228571   \n",
       "4                0.062500                0.117647                0.470588   \n",
       "\n",
       "   A12_intTarget_2.0_mean  A12_intTarget_3.0_mean  A12_intTarget_4.0_mean  \\\n",
       "0                0.392500                0.095000                0.057500   \n",
       "1                0.401180                0.082596                0.067847   \n",
       "2                0.390244                0.097561                0.032520   \n",
       "3                0.471429                0.107143                0.050000   \n",
       "4                0.294118                0.117647                0.000000   \n",
       "\n",
       "   A14_intTarget_0.0_mean  A14_intTarget_1.0_mean  A14_intTarget_2.0_mean  \\\n",
       "0                0.121951                0.341463                0.317073   \n",
       "1                0.130268                0.295019                0.406130   \n",
       "2                0.214286                0.214286                0.357143   \n",
       "3                0.173913                0.285714                0.354037   \n",
       "4                0.093750                0.312500                0.437500   \n",
       "\n",
       "   A14_intTarget_3.0_mean  A14_intTarget_4.0_mean  A16_intTarget_0.0_mean  \\\n",
       "0                0.121951                0.048780                0.121951   \n",
       "1                0.084291                0.076628                0.130268   \n",
       "2                0.166667                0.047619                0.214286   \n",
       "3                0.105590                0.074534                0.173913   \n",
       "4                0.093750                0.062500                0.093750   \n",
       "\n",
       "   A16_intTarget_1.0_mean  A16_intTarget_2.0_mean  A16_intTarget_3.0_mean  \\\n",
       "0                0.341463                0.317073                0.121951   \n",
       "1                0.295019                0.406130                0.084291   \n",
       "2                0.214286                0.357143                0.166667   \n",
       "3                0.285714                0.354037                0.105590   \n",
       "4                0.312500                0.437500                0.093750   \n",
       "\n",
       "   A16_intTarget_4.0_mean  A17_intTarget_0.0_mean  A17_intTarget_1.0_mean  \\\n",
       "0                0.048780                0.195402                0.258621   \n",
       "1                0.076628                0.160294                0.302941   \n",
       "2                0.047619                0.121739                0.226087   \n",
       "3                0.074534                0.227723                0.277228   \n",
       "4                0.062500                0.112150                0.299065   \n",
       "\n",
       "   A17_intTarget_2.0_mean  A17_intTarget_3.0_mean  A17_intTarget_4.0_mean  \\\n",
       "0                0.425287                0.074713                0.028736   \n",
       "1                0.388235                0.080882                0.060294   \n",
       "2                0.452174                0.121739                0.069565   \n",
       "3                0.356436                0.069307                0.049505   \n",
       "4                0.336449                0.177570                0.056075   \n",
       "\n",
       "   A24_intTarget_0.0_mean  A24_intTarget_1.0_mean  A24_intTarget_2.0_mean  \\\n",
       "0                0.176471                0.352941                0.411765   \n",
       "1                0.128405                0.311284                0.389105   \n",
       "2                0.196970                0.242424                0.439394   \n",
       "3                0.178344                0.267516                0.363057   \n",
       "4                0.134021                0.302405                0.422680   \n",
       "\n",
       "   A24_intTarget_3.0_mean  A24_intTarget_4.0_mean  A25_intTarget_0.0_mean  \\\n",
       "0                0.058824                0.000000                0.130435   \n",
       "1                0.085603                0.077821                0.159851   \n",
       "2                0.090909                0.015152                0.189873   \n",
       "3                0.108280                0.076433                0.168605   \n",
       "4                0.079038                0.054983                0.145161   \n",
       "\n",
       "   A25_intTarget_1.0_mean  A25_intTarget_2.0_mean  A25_intTarget_3.0_mean  \\\n",
       "0                0.260870                0.434783                0.000000   \n",
       "1                0.284387                0.390335                0.100372   \n",
       "2                0.291139                0.430380                0.037975   \n",
       "3                0.275194                0.393411                0.096899   \n",
       "4                0.290323                0.381720                0.086022   \n",
       "\n",
       "   A25_intTarget_4.0_mean  A26_intTarget_0.0_mean  A26_intTarget_1.0_mean  \\\n",
       "0                0.130435                0.047619                0.380952   \n",
       "1                0.055762                0.129771                0.305344   \n",
       "2                0.050633                0.266667                0.222222   \n",
       "3                0.050388                0.176101                0.264151   \n",
       "4                0.091398                0.236842                0.263158   \n",
       "\n",
       "   A26_intTarget_2.0_mean  A26_intTarget_3.0_mean  A26_intTarget_4.0_mean  \\\n",
       "0                0.476190                0.095238                0.000000   \n",
       "1                0.396947                0.080153                0.076336   \n",
       "2                0.355556                0.133333                0.022222   \n",
       "3                0.371069                0.113208                0.075472   \n",
       "4                0.368421                0.078947                0.052632   \n",
       "\n",
       "   A27_intTarget_0.0_mean  A27_intTarget_1.0_mean  A27_intTarget_2.0_mean  \\\n",
       "0                0.086957                0.304348                0.413043   \n",
       "1                0.151515                0.271132                0.408293   \n",
       "2                0.167273                0.294545                0.385455   \n",
       "3                0.165049                0.237864                0.441748   \n",
       "4                0.166667                0.296296                0.370370   \n",
       "\n",
       "   A27_intTarget_3.0_mean  A27_intTarget_4.0_mean  B5_intTarget_0.0_mean  \\\n",
       "0                0.043478                0.130435               0.142857   \n",
       "1                0.095694                0.066986               0.121622   \n",
       "2                0.087273                0.050909               0.152091   \n",
       "3                0.101942                0.038835               0.174863   \n",
       "4                0.129630                0.018519               0.107143   \n",
       "\n",
       "   B5_intTarget_1.0_mean  B5_intTarget_2.0_mean  B5_intTarget_3.0_mean  \\\n",
       "0               0.190476               0.476190               0.190476   \n",
       "1               0.310811               0.387387               0.090090   \n",
       "2               0.304183               0.418251               0.076046   \n",
       "3               0.278689               0.382514               0.092896   \n",
       "4               0.250000               0.464286               0.107143   \n",
       "\n",
       "   B5_intTarget_4.0_mean  B6_intTarget_0.0_mean  B6_intTarget_1.0_mean  \\\n",
       "0               0.000000               0.180272               0.238095   \n",
       "1               0.081081               0.154088               0.273585   \n",
       "2               0.041825               0.166667               0.500000   \n",
       "3               0.071038               0.128205               0.307692   \n",
       "4               0.071429               0.153846               0.307692   \n",
       "\n",
       "   B6_intTarget_2.0_mean  B6_intTarget_3.0_mean  B6_intTarget_4.0_mean  \\\n",
       "0               0.394558               0.102041               0.071429   \n",
       "1               0.410377               0.091195               0.064465   \n",
       "2               0.333333               0.000000               0.000000   \n",
       "3               0.307692               0.076923               0.153846   \n",
       "4               0.538462               0.000000               0.000000   \n",
       "\n",
       "   B7_intTarget_0.0_mean  B7_intTarget_1.0_mean  B7_intTarget_2.0_mean  \\\n",
       "0               0.173913               0.304348               0.434783   \n",
       "1               0.173913               0.217391               0.347826   \n",
       "2               0.142857               0.308756               0.368664   \n",
       "3               0.272727               0.181818               0.363636   \n",
       "4               0.165680               0.272189               0.384615   \n",
       "\n",
       "   B7_intTarget_3.0_mean  B7_intTarget_4.0_mean  ID_intTarget_0.0_mean  \\\n",
       "0               0.043478               0.043478                      1   \n",
       "1               0.130435               0.130435                      0   \n",
       "2               0.092166               0.078341                      0   \n",
       "3               0.181818               0.000000                      0   \n",
       "4               0.100592               0.076923                      0   \n",
       "\n",
       "   ID_intTarget_1.0_mean  ID_intTarget_2.0_mean  ID_intTarget_3.0_mean  \\\n",
       "0                      0                      0                      0   \n",
       "1                      1                      0                      0   \n",
       "2                      0                      1                      0   \n",
       "3                      1                      0                      0   \n",
       "4                      0                      0                      0   \n",
       "\n",
       "   ID_intTarget_4.0_mean  \n",
       "0                      0  \n",
       "1                      0  \n",
       "2                      0  \n",
       "3                      0  \n",
       "4                      1  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myFeval(preds, xgbtrain):\n",
    "    label = xgbtrain.get_label()\n",
    "    score = mean_squared_error(label,preds)*0.5\n",
    "    return 'myFeval',score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's l2: 0.00025499\tvalid_1's l2: 0.000270749\n",
      "[400]\ttraining's l2: 0.000197672\tvalid_1's l2: 0.000225229\n",
      "[600]\ttraining's l2: 0.000170057\tvalid_1's l2: 0.000213595\n",
      "[800]\ttraining's l2: 0.000153779\tvalid_1's l2: 0.000209849\n",
      "[1000]\ttraining's l2: 0.000142993\tvalid_1's l2: 0.000208749\n",
      "[1200]\ttraining's l2: 0.000134856\tvalid_1's l2: 0.000207857\n",
      "[1400]\ttraining's l2: 0.000128503\tvalid_1's l2: 0.000207811\n",
      "[1600]\ttraining's l2: 0.000123659\tvalid_1's l2: 0.000207162\n",
      "[1800]\ttraining's l2: 0.000119661\tvalid_1's l2: 0.00020693\n",
      "Early stopping, best iteration is:\n",
      "[1715]\ttraining's l2: 0.000121303\tvalid_1's l2: 0.000206845\n",
      "fold n°2\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's l2: 0.000244175\tvalid_1's l2: 0.000271907\n",
      "[400]\ttraining's l2: 0.000189001\tvalid_1's l2: 0.000253665\n",
      "[600]\ttraining's l2: 0.000161586\tvalid_1's l2: 0.00024844\n",
      "[800]\ttraining's l2: 0.000145991\tvalid_1's l2: 0.000247133\n",
      "Early stopping, best iteration is:\n",
      "[798]\ttraining's l2: 0.000146094\tvalid_1's l2: 0.000247088\n",
      "fold n°3\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's l2: 0.000261114\tvalid_1's l2: 0.000352411\n",
      "[400]\ttraining's l2: 0.000203589\tvalid_1's l2: 0.000292335\n",
      "[600]\ttraining's l2: 0.000174791\tvalid_1's l2: 0.00026772\n",
      "[800]\ttraining's l2: 0.00015717\tvalid_1's l2: 0.000255757\n",
      "[1000]\ttraining's l2: 0.000145068\tvalid_1's l2: 0.000248301\n",
      "[1200]\ttraining's l2: 0.000135884\tvalid_1's l2: 0.000243547\n",
      "[1400]\ttraining's l2: 0.000129023\tvalid_1's l2: 0.000240221\n",
      "[1600]\ttraining's l2: 0.000123375\tvalid_1's l2: 0.000238114\n",
      "[1800]\ttraining's l2: 0.000118712\tvalid_1's l2: 0.000236198\n",
      "[2000]\ttraining's l2: 0.000114993\tvalid_1's l2: 0.000235201\n",
      "[2200]\ttraining's l2: 0.000111684\tvalid_1's l2: 0.000234815\n",
      "[2400]\ttraining's l2: 0.00010893\tvalid_1's l2: 0.000234264\n",
      "Early stopping, best iteration is:\n",
      "[2363]\ttraining's l2: 0.000109448\tvalid_1's l2: 0.000234072\n",
      "fold n°4\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's l2: 0.000265957\tvalid_1's l2: 0.000281888\n",
      "[400]\ttraining's l2: 0.000201194\tvalid_1's l2: 0.000242186\n",
      "[600]\ttraining's l2: 0.000168669\tvalid_1's l2: 0.000225752\n",
      "[800]\ttraining's l2: 0.000151752\tvalid_1's l2: 0.000220448\n",
      "[1000]\ttraining's l2: 0.000140911\tvalid_1's l2: 0.000219056\n",
      "[1200]\ttraining's l2: 0.000132544\tvalid_1's l2: 0.000217627\n",
      "[1400]\ttraining's l2: 0.000126239\tvalid_1's l2: 0.000216802\n",
      "[1600]\ttraining's l2: 0.000121156\tvalid_1's l2: 0.000215826\n",
      "[1800]\ttraining's l2: 0.000116536\tvalid_1's l2: 0.000215052\n",
      "Early stopping, best iteration is:\n",
      "[1812]\ttraining's l2: 0.000116273\tvalid_1's l2: 0.000214919\n",
      "fold n°5\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's l2: 0.000265204\tvalid_1's l2: 0.000271345\n",
      "[400]\ttraining's l2: 0.000201719\tvalid_1's l2: 0.000216549\n",
      "[600]\ttraining's l2: 0.000172835\tvalid_1's l2: 0.000199718\n",
      "[800]\ttraining's l2: 0.00015595\tvalid_1's l2: 0.000194643\n",
      "[1000]\ttraining's l2: 0.000144545\tvalid_1's l2: 0.000193079\n",
      "[1200]\ttraining's l2: 0.000136177\tvalid_1's l2: 0.000192424\n",
      "[1400]\ttraining's l2: 0.000129941\tvalid_1's l2: 0.000191891\n",
      "[1600]\ttraining's l2: 0.000124615\tvalid_1's l2: 0.00019138\n",
      "Early stopping, best iteration is:\n",
      "[1610]\ttraining's l2: 0.000124376\tvalid_1's l2: 0.000191377\n",
      "CV score: 0.00021885\n"
     ]
    }
   ],
   "source": [
    "param = {'num_leaves': 120,\n",
    "         'min_data_in_leaf': 30, \n",
    "         'objective':'regression',\n",
    "         'max_depth': -1,\n",
    "         'learning_rate': 0.01,\n",
    "         \"min_child_samples\": 30,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.9,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.9 ,\n",
    "         \"bagging_seed\": 11,\n",
    "         \"metric\": 'mse',\n",
    "         \"lambda_l1\": 0.1,\n",
    "         \"verbosity\": -1}\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=2018)\n",
    "oof_lgb = np.zeros(len(train))\n",
    "predictions_lgb = np.zeros(len(test))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    trn_data = lgb.Dataset(X_train[trn_idx], y_train[trn_idx])\n",
    "    val_data = lgb.Dataset(X_train[val_idx], y_train[val_idx])\n",
    "\n",
    "    num_round = 10000\n",
    "    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=200, early_stopping_rounds = 100)\n",
    "    oof_lgb[val_idx] = clf.predict(X_train[val_idx], num_iteration=clf.best_iteration)\n",
    "    \n",
    "    predictions_lgb += clf.predict(X_test, num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_lgb, target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "[0]\ttrain-rmse:0.422934\tvalid_data-rmse:0.423824\ttrain-myFeval:0.089436\tvalid_data-myFeval:0.089813\n",
      "Multiple eval metrics have been passed: 'valid_data-myFeval' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-myFeval hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:0.257099\tvalid_data-rmse:0.258054\ttrain-myFeval:0.03305\tvalid_data-myFeval:0.033296\n",
      "[200]\ttrain-rmse:0.156783\tvalid_data-rmse:0.157707\ttrain-myFeval:0.012291\tvalid_data-myFeval:0.012436\n",
      "[300]\ttrain-rmse:0.096245\tvalid_data-rmse:0.097064\ttrain-myFeval:0.004632\tvalid_data-myFeval:0.004711\n",
      "[400]\ttrain-rmse:0.059876\tvalid_data-rmse:0.060683\ttrain-myFeval:0.001793\tvalid_data-myFeval:0.001841\n",
      "[500]\ttrain-rmse:0.038168\tvalid_data-rmse:0.039171\ttrain-myFeval:0.000728\tvalid_data-myFeval:0.000767\n",
      "[600]\ttrain-rmse:0.02526\tvalid_data-rmse:0.026852\ttrain-myFeval:0.000319\tvalid_data-myFeval:0.000361\n",
      "[700]\ttrain-rmse:0.017518\tvalid_data-rmse:0.02007\ttrain-myFeval:0.000153\tvalid_data-myFeval:0.000201\n",
      "[800]\ttrain-rmse:0.01286\tvalid_data-rmse:0.016571\ttrain-myFeval:8.3e-05\tvalid_data-myFeval:0.000137\n",
      "[900]\ttrain-rmse:0.010027\tvalid_data-rmse:0.014899\ttrain-myFeval:5e-05\tvalid_data-myFeval:0.000111\n",
      "[1000]\ttrain-rmse:0.008341\tvalid_data-rmse:0.014116\ttrain-myFeval:3.5e-05\tvalid_data-myFeval:0.0001\n",
      "[1100]\ttrain-rmse:0.007244\tvalid_data-rmse:0.013775\ttrain-myFeval:2.6e-05\tvalid_data-myFeval:9.5e-05\n",
      "[1200]\ttrain-rmse:0.006434\tvalid_data-rmse:0.013607\ttrain-myFeval:2.1e-05\tvalid_data-myFeval:9.3e-05\n",
      "[1300]\ttrain-rmse:0.00583\tvalid_data-rmse:0.013535\ttrain-myFeval:1.7e-05\tvalid_data-myFeval:9.2e-05\n",
      "[1400]\ttrain-rmse:0.005312\tvalid_data-rmse:0.013506\ttrain-myFeval:1.4e-05\tvalid_data-myFeval:9.1e-05\n",
      "[1500]\ttrain-rmse:0.004838\tvalid_data-rmse:0.013488\ttrain-myFeval:1.2e-05\tvalid_data-myFeval:9.1e-05\n",
      "Stopping. Best iteration:\n",
      "[1315]\ttrain-rmse:0.005746\tvalid_data-rmse:0.013527\ttrain-myFeval:1.7e-05\tvalid_data-myFeval:9.1e-05\n",
      "\n",
      "fold n°2\n",
      "[0]\ttrain-rmse:0.424044\tvalid_data-rmse:0.419358\ttrain-myFeval:0.089907\tvalid_data-myFeval:0.08793\n",
      "Multiple eval metrics have been passed: 'valid_data-myFeval' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-myFeval hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:0.257771\tvalid_data-rmse:0.253801\ttrain-myFeval:0.033223\tvalid_data-myFeval:0.032208\n",
      "[200]\ttrain-rmse:0.157193\tvalid_data-rmse:0.153957\ttrain-myFeval:0.012355\tvalid_data-myFeval:0.011851\n",
      "[300]\ttrain-rmse:0.096486\tvalid_data-rmse:0.093749\ttrain-myFeval:0.004655\tvalid_data-myFeval:0.004394\n",
      "[400]\ttrain-rmse:0.059953\tvalid_data-rmse:0.057686\ttrain-myFeval:0.001797\tvalid_data-myFeval:0.001664\n",
      "[500]\ttrain-rmse:0.038132\tvalid_data-rmse:0.036462\ttrain-myFeval:0.000727\tvalid_data-myFeval:0.000665\n",
      "[600]\ttrain-rmse:0.025152\tvalid_data-rmse:0.024721\ttrain-myFeval:0.000316\tvalid_data-myFeval:0.000306\n",
      "[700]\ttrain-rmse:0.01737\tvalid_data-rmse:0.018812\ttrain-myFeval:0.000151\tvalid_data-myFeval:0.000177\n",
      "[800]\ttrain-rmse:0.012615\tvalid_data-rmse:0.01619\ttrain-myFeval:8e-05\tvalid_data-myFeval:0.000131\n",
      "[900]\ttrain-rmse:0.009766\tvalid_data-rmse:0.01518\ttrain-myFeval:4.8e-05\tvalid_data-myFeval:0.000115\n",
      "[1000]\ttrain-rmse:0.00803\tvalid_data-rmse:0.014865\ttrain-myFeval:3.2e-05\tvalid_data-myFeval:0.00011\n",
      "[1100]\ttrain-rmse:0.006937\tvalid_data-rmse:0.014813\ttrain-myFeval:2.4e-05\tvalid_data-myFeval:0.00011\n",
      "[1200]\ttrain-rmse:0.006173\tvalid_data-rmse:0.014843\ttrain-myFeval:1.9e-05\tvalid_data-myFeval:0.00011\n",
      "Stopping. Best iteration:\n",
      "[1000]\ttrain-rmse:0.00803\tvalid_data-rmse:0.014865\ttrain-myFeval:3.2e-05\tvalid_data-myFeval:0.00011\n",
      "\n",
      "fold n°3\n",
      "[0]\ttrain-rmse:0.422402\tvalid_data-rmse:0.425952\ttrain-myFeval:0.089212\tvalid_data-myFeval:0.090718\n",
      "Multiple eval metrics have been passed: 'valid_data-myFeval' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-myFeval hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:0.256764\tvalid_data-rmse:0.260059\ttrain-myFeval:0.032964\tvalid_data-myFeval:0.033815\n",
      "[200]\ttrain-rmse:0.156562\tvalid_data-rmse:0.159727\ttrain-myFeval:0.012256\tvalid_data-myFeval:0.012756\n",
      "[300]\ttrain-rmse:0.096054\tvalid_data-rmse:0.099125\ttrain-myFeval:0.004613\tvalid_data-myFeval:0.004913\n",
      "[400]\ttrain-rmse:0.059691\tvalid_data-rmse:0.062639\ttrain-myFeval:0.001782\tvalid_data-myFeval:0.001962\n",
      "[500]\ttrain-rmse:0.038016\tvalid_data-rmse:0.040836\ttrain-myFeval:0.000723\tvalid_data-myFeval:0.000834\n",
      "[600]\ttrain-rmse:0.025166\tvalid_data-rmse:0.028252\ttrain-myFeval:0.000317\tvalid_data-myFeval:0.000399\n",
      "[700]\ttrain-rmse:0.017518\tvalid_data-rmse:0.021459\ttrain-myFeval:0.000153\tvalid_data-myFeval:0.00023\n",
      "[800]\ttrain-rmse:0.012843\tvalid_data-rmse:0.018004\ttrain-myFeval:8.2e-05\tvalid_data-myFeval:0.000162\n",
      "[900]\ttrain-rmse:0.00999\tvalid_data-rmse:0.01637\ttrain-myFeval:5e-05\tvalid_data-myFeval:0.000134\n",
      "[1000]\ttrain-rmse:0.008219\tvalid_data-rmse:0.015636\ttrain-myFeval:3.4e-05\tvalid_data-myFeval:0.000122\n",
      "[1100]\ttrain-rmse:0.007047\tvalid_data-rmse:0.015319\ttrain-myFeval:2.5e-05\tvalid_data-myFeval:0.000117\n",
      "[1200]\ttrain-rmse:0.006223\tvalid_data-rmse:0.015176\ttrain-myFeval:1.9e-05\tvalid_data-myFeval:0.000115\n",
      "[1300]\ttrain-rmse:0.005526\tvalid_data-rmse:0.015125\ttrain-myFeval:1.5e-05\tvalid_data-myFeval:0.000114\n",
      "[1400]\ttrain-rmse:0.004998\tvalid_data-rmse:0.015104\ttrain-myFeval:1.2e-05\tvalid_data-myFeval:0.000114\n",
      "Stopping. Best iteration:\n",
      "[1277]\ttrain-rmse:0.005686\tvalid_data-rmse:0.015132\ttrain-myFeval:1.6e-05\tvalid_data-myFeval:0.000114\n",
      "\n",
      "fold n°4\n",
      "[0]\ttrain-rmse:0.422949\tvalid_data-rmse:0.423757\ttrain-myFeval:0.089443\tvalid_data-myFeval:0.089785\n",
      "Multiple eval metrics have been passed: 'valid_data-myFeval' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-myFeval hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:0.257129\tvalid_data-rmse:0.257158\ttrain-myFeval:0.033058\tvalid_data-myFeval:0.033065\n",
      "[200]\ttrain-rmse:0.156786\tvalid_data-rmse:0.156321\ttrain-myFeval:0.012291\tvalid_data-myFeval:0.012218\n",
      "[300]\ttrain-rmse:0.096219\tvalid_data-rmse:0.095358\ttrain-myFeval:0.004629\tvalid_data-myFeval:0.004547\n",
      "[400]\ttrain-rmse:0.059839\tvalid_data-rmse:0.059068\ttrain-myFeval:0.00179\tvalid_data-myFeval:0.001744\n",
      "[500]\ttrain-rmse:0.038133\tvalid_data-rmse:0.037821\ttrain-myFeval:0.000727\tvalid_data-myFeval:0.000715\n",
      "[600]\ttrain-rmse:0.025246\tvalid_data-rmse:0.025801\ttrain-myFeval:0.000319\tvalid_data-myFeval:0.000333\n",
      "[700]\ttrain-rmse:0.017523\tvalid_data-rmse:0.019449\ttrain-myFeval:0.000154\tvalid_data-myFeval:0.000189\n",
      "[800]\ttrain-rmse:0.012793\tvalid_data-rmse:0.016327\ttrain-myFeval:8.2e-05\tvalid_data-myFeval:0.000133\n",
      "[900]\ttrain-rmse:0.009909\tvalid_data-rmse:0.014879\ttrain-myFeval:4.9e-05\tvalid_data-myFeval:0.000111\n",
      "[1000]\ttrain-rmse:0.008142\tvalid_data-rmse:0.014288\ttrain-myFeval:3.3e-05\tvalid_data-myFeval:0.000102\n",
      "[1100]\ttrain-rmse:0.006995\tvalid_data-rmse:0.014022\ttrain-myFeval:2.4e-05\tvalid_data-myFeval:9.8e-05\n",
      "[1200]\ttrain-rmse:0.006183\tvalid_data-rmse:0.013913\ttrain-myFeval:1.9e-05\tvalid_data-myFeval:9.7e-05\n",
      "[1300]\ttrain-rmse:0.005569\tvalid_data-rmse:0.013886\ttrain-myFeval:1.6e-05\tvalid_data-myFeval:9.6e-05\n",
      "[1400]\ttrain-rmse:0.005053\tvalid_data-rmse:0.013888\ttrain-myFeval:1.3e-05\tvalid_data-myFeval:9.6e-05\n",
      "Stopping. Best iteration:\n",
      "[1261]\ttrain-rmse:0.005787\tvalid_data-rmse:0.013892\ttrain-myFeval:1.7e-05\tvalid_data-myFeval:9.6e-05\n",
      "\n",
      "fold n°5\n",
      "[0]\ttrain-rmse:0.423229\tvalid_data-rmse:0.422639\ttrain-myFeval:0.089561\tvalid_data-myFeval:0.089312\n",
      "Multiple eval metrics have been passed: 'valid_data-myFeval' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-myFeval hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:0.257293\tvalid_data-rmse:0.257017\ttrain-myFeval:0.0331\tvalid_data-myFeval:0.033029\n",
      "[200]\ttrain-rmse:0.156921\tvalid_data-rmse:0.156702\ttrain-myFeval:0.012312\tvalid_data-myFeval:0.012278\n",
      "[300]\ttrain-rmse:0.09632\tvalid_data-rmse:0.096312\ttrain-myFeval:0.004639\tvalid_data-myFeval:0.004638\n",
      "[400]\ttrain-rmse:0.05991\tvalid_data-rmse:0.060152\ttrain-myFeval:0.001795\tvalid_data-myFeval:0.001809\n",
      "[500]\ttrain-rmse:0.038179\tvalid_data-rmse:0.038835\ttrain-myFeval:0.000729\tvalid_data-myFeval:0.000754\n",
      "[600]\ttrain-rmse:0.025296\tvalid_data-rmse:0.026705\ttrain-myFeval:0.00032\tvalid_data-myFeval:0.000357\n",
      "[700]\ttrain-rmse:0.01761\tvalid_data-rmse:0.020184\ttrain-myFeval:0.000155\tvalid_data-myFeval:0.000204\n",
      "[800]\ttrain-rmse:0.01296\tvalid_data-rmse:0.016827\ttrain-myFeval:8.4e-05\tvalid_data-myFeval:0.000142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[900]\ttrain-rmse:0.010154\tvalid_data-rmse:0.01514\ttrain-myFeval:5.2e-05\tvalid_data-myFeval:0.000115\n",
      "[1000]\ttrain-rmse:0.008497\tvalid_data-rmse:0.014324\ttrain-myFeval:3.6e-05\tvalid_data-myFeval:0.000103\n",
      "[1100]\ttrain-rmse:0.007417\tvalid_data-rmse:0.013922\ttrain-myFeval:2.8e-05\tvalid_data-myFeval:9.7e-05\n",
      "[1200]\ttrain-rmse:0.006625\tvalid_data-rmse:0.013765\ttrain-myFeval:2.2e-05\tvalid_data-myFeval:9.5e-05\n",
      "[1300]\ttrain-rmse:0.005995\tvalid_data-rmse:0.01368\ttrain-myFeval:1.8e-05\tvalid_data-myFeval:9.4e-05\n",
      "[1400]\ttrain-rmse:0.005446\tvalid_data-rmse:0.013657\ttrain-myFeval:1.5e-05\tvalid_data-myFeval:9.3e-05\n",
      "[1500]\ttrain-rmse:0.004971\tvalid_data-rmse:0.013653\ttrain-myFeval:1.2e-05\tvalid_data-myFeval:9.3e-05\n",
      "Stopping. Best iteration:\n",
      "[1305]\ttrain-rmse:0.005961\tvalid_data-rmse:0.013673\ttrain-myFeval:1.8e-05\tvalid_data-myFeval:9.3e-05\n",
      "\n",
      "CV score: 0.00020256\n"
     ]
    }
   ],
   "source": [
    "##### xgb\n",
    "xgb_params = {'eta': 0.005, 'max_depth': 10, 'subsample': 0.8, 'colsample_bytree': 0.8, \n",
    "          'objective': 'reg:linear', 'eval_metric': 'rmse', 'silent': True, 'nthread': 4}\n",
    "\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=2018)\n",
    "oof_xgb = np.zeros(len(train))\n",
    "predictions_xgb = np.zeros(len(test))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    trn_data = xgb.DMatrix(X_train[trn_idx], y_train[trn_idx])\n",
    "    val_data = xgb.DMatrix(X_train[val_idx], y_train[val_idx])\n",
    "\n",
    "    watchlist = [(trn_data, 'train'), (val_data, 'valid_data')]\n",
    "    clf = xgb.train(dtrain=trn_data, num_boost_round=20000, feval = myFeval, evals=watchlist, early_stopping_rounds=200, verbose_eval=100, params=xgb_params)\n",
    "    oof_xgb[val_idx] = clf.predict(xgb.DMatrix(X_train[val_idx]), ntree_limit=clf.best_ntree_limit)\n",
    "    predictions_xgb += clf.predict(xgb.DMatrix(X_test), ntree_limit=clf.best_ntree_limit) / folds.n_splits\n",
    "    \n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_xgb, target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "fold 3\n",
      "fold 4\n",
      "fold 5\n",
      "fold 6\n",
      "fold 7\n",
      "fold 8\n",
      "fold 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0001989710003463426"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将lgb和xgb的结果进行stacking\n",
    "train_stack = np.vstack([oof_lgb,oof_xgb]).transpose()\n",
    "test_stack = np.vstack([predictions_lgb, predictions_xgb]).transpose()\n",
    "\n",
    "folds_stack = RepeatedKFold(n_splits=5, n_repeats=2, random_state=4590)\n",
    "oof_stack = np.zeros(train_stack.shape[0])\n",
    "predictions = np.zeros(test_stack.shape[0])\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds_stack.split(train_stack,target)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    trn_data, trn_y = train_stack[trn_idx], target.iloc[trn_idx].values\n",
    "    val_data, val_y = train_stack[val_idx], target.iloc[val_idx].values\n",
    "    \n",
    "    clf_3 = BayesianRidge()\n",
    "    clf_3.fit(trn_data, trn_y)\n",
    "    \n",
    "    oof_stack[val_idx] = clf_3.predict(val_data)\n",
    "    predictions += clf_3.predict(test_stack) / 10\n",
    "    \n",
    "mean_squared_error(target.values, oof_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('submission.csv', header=None)\n",
    "sub_df[1] = predictions\n",
    "sub_df[1] = sub_df[1].apply(lambda x:round(x, 3))\n",
    "sub_df.to_csv(\"ly_submission.csv\", index=False, header=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
